{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1d24f0bc",
   "metadata": {},
   "source": [
    "# Bookie Evaluator - Backtesting & ML Training\n",
    "\n",
    "This notebook provides tools for backtesting strategies and training machine learning models to improve prediction accuracy for football match outcomes.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Data Loading**: Import historical match data from your bookie_evaluator database\n",
    "- **Model Training**: Train and evaluate multiple ML models\n",
    "- **Hyperparameter Tuning**: Optimize model parameters\n",
    "- **Backtesting**: Run simulations on historical data\n",
    "- **Visualization**: Analyze performance metrics and feature importance\n",
    "\n",
    "Let's begin by setting up our environment and importing the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bfc9f55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment set up successfully!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Add project root to Python path\n",
    "module_path = os.path.abspath(os.path.join('.'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "# Import project modules\n",
    "from models.odds_analyzer import OddsAnalyzer\n",
    "from ml.model import FootballMatchPredictor\n",
    "from utils.file_utils import load_all_match_analyses\n",
    "\n",
    "# Initialize core components\n",
    "data_dir = os.path.join(os.path.dirname(os.path.abspath('.')), 'data')\n",
    "analyzer = OddsAnalyzer(data_dir=data_dir)\n",
    "ml_predictor = FootballMatchPredictor(data_dir=data_dir, model_dir='ml/models')\n",
    "\n",
    "print(f\"Environment set up successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15194aa",
   "metadata": {},
   "source": [
    "## Data Loading and Preparation\n",
    "\n",
    "First, we'll load the historical match data from our database. This includes both the match analyses and the recorded results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a68fa8cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading historical match data...\n",
      "Found 0 match analyses\n",
      "Found 0 recorded match results\n",
      "Combined 0 matches with both analysis and results\n",
      "No historical match data with results found. Please record some match results first.\n"
     ]
    }
   ],
   "source": [
    "def load_training_data():\n",
    "    \"\"\"Load and prepare historical match data for training and backtesting\"\"\"\n",
    "    print(\"Loading historical match data...\")\n",
    "    \n",
    "    # Load all match analyses\n",
    "    raw_matches = load_all_match_analyses(data_dir)\n",
    "    print(f\"Found {len(raw_matches)} match analyses\")\n",
    "    \n",
    "    # Get completed matches from outcome performance data\n",
    "    outcome_matches = analyzer.outcome_performance.get('matches', [])\n",
    "    print(f\"Found {len(outcome_matches)} recorded match results\")\n",
    "    \n",
    "    # Combine data to ensure we have match analyses with results\n",
    "    all_matches = []\n",
    "    for match in raw_matches:\n",
    "        match_id = match.get('match_id', '')\n",
    "        outcome_match = next((m for m in outcome_matches if m.get('match_id') == match_id), None)\n",
    "        \n",
    "        if outcome_match:\n",
    "            # Add the actual result to the match analysis\n",
    "            match['actual_result'] = outcome_match.get('actual_result')\n",
    "            all_matches.append(match)\n",
    "    \n",
    "    print(f\"Combined {len(all_matches)} matches with both analysis and results\")\n",
    "    \n",
    "    # Convert to a pandas DataFrame for easier manipulation\n",
    "    match_df = pd.DataFrame(all_matches)\n",
    "    \n",
    "    return all_matches, match_df\n",
    "\n",
    "# Load the data\n",
    "all_matches, match_df = load_training_data()\n",
    "\n",
    "if not all_matches:\n",
    "    print(\"No historical match data with results found. Please record some match results first.\")\n",
    "else:\n",
    "    # Display basic statistics\n",
    "    if 'actual_result' in match_df.columns:\n",
    "        result_counts = match_df['actual_result'].value_counts()\n",
    "        print(\"\\nOutcome distribution:\")\n",
    "        print(result_counts)\n",
    "        \n",
    "        # Create pie chart\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.pie(result_counts, labels=result_counts.index, autopct='%1.1f%%', startangle=90)\n",
    "        plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "        plt.title('Distribution of Match Outcomes')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d173718",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis\n",
    "\n",
    "Let's explore the historical data to understand the patterns and relationships between bookmaker odds and match outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed6cc6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_bookmaker_dataframe(matches):\n",
    "    \"\"\"Extract a flat DataFrame from nested bookmaker data\"\"\"\n",
    "    rows = []\n",
    "    \n",
    "    for match in matches:\n",
    "        match_id = match.get('match_id', '')\n",
    "        home_team = match.get('home_team', '')\n",
    "        away_team = match.get('away_team', '')\n",
    "        actual_result = match.get('actual_result', None)\n",
    "        competition = match.get('competition', '')\n",
    "        match_date = match.get('match_date', '')\n",
    "        \n",
    "        for bookie in match.get('bookmakers', []):\n",
    "            row = {\n",
    "                'match_id': match_id,\n",
    "                'home_team': home_team,\n",
    "                'away_team': away_team,\n",
    "                'competition': competition,\n",
    "                'match_date': match_date,\n",
    "                'actual_result': actual_result,\n",
    "                'bookmaker': bookie.get('name', ''),\n",
    "                'home_odds': bookie.get('home_odds', 0),\n",
    "                'draw_odds': bookie.get('draw_odds', 0),\n",
    "                'away_odds': bookie.get('away_odds', 0),\n",
    "                'implied_home_prob': bookie.get('implied_home_prob', 0),\n",
    "                'implied_draw_prob': bookie.get('implied_draw_prob', 0),\n",
    "                'implied_away_prob': bookie.get('implied_away_prob', 0),\n",
    "                'margin': bookie.get('margin', 0)\n",
    "            }\n",
    "            rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Extract flat DataFrame for analysis if we have data\n",
    "if all_matches:\n",
    "    bookie_df = extract_bookmaker_dataframe(all_matches)\n",
    "    \n",
    "    # Display basic information\n",
    "    print(f\"Bookmaker data shape: {bookie_df.shape}\")\n",
    "    print(f\"Unique bookmakers: {bookie_df['bookmaker'].nunique()}\")\n",
    "    print(f\"Unique matches: {bookie_df['match_id'].nunique()}\")\n",
    "    \n",
    "    # Display the first few rows\n",
    "    bookie_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c174bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'bookie_df' in locals() and not bookie_df.empty:\n",
    "    # Analyze bookmaker implied probabilities vs actual outcomes\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Create subplots for home, draw, away\n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.boxplot(x=\"actual_result\", y=\"implied_home_prob\", data=bookie_df)\n",
    "    plt.title(\"Home Win Probability by Actual Result\")\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.boxplot(x=\"actual_result\", y=\"implied_draw_prob\", data=bookie_df)\n",
    "    plt.title(\"Draw Probability by Actual Result\")\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.boxplot(x=\"actual_result\", y=\"implied_away_prob\", data=bookie_df)\n",
    "    plt.title(\"Away Win Probability by Actual Result\")\n",
    "    plt.ylim(0, 1)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Check average probabilities by actual result\n",
    "    avg_probs = bookie_df.groupby('actual_result')[\n",
    "        ['implied_home_prob', 'implied_draw_prob', 'implied_away_prob']\n",
    "    ].mean().reset_index()\n",
    "    \n",
    "    avg_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78417ea4",
   "metadata": {},
   "source": [
    "## Machine Learning Model Training\n",
    "\n",
    "Now let's train different machine learning models and evaluate their performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66c86e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_models(matches, test_size=0.2, random_state=42):\n",
    "    \"\"\"Train ML models and evaluate their performance\"\"\"\n",
    "    print(\"Preparing training data...\")\n",
    "    X, y = ml_predictor.prepare_training_data(matches)\n",
    "    \n",
    "    if X.empty or y.empty:\n",
    "        print(\"Failed to extract features from historical matches.\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Training data prepared: {X.shape[0]} samples with {X.shape[1]} features\")\n",
    "    \n",
    "    print(\"\\nTraining machine learning models...\")\n",
    "    results = ml_predictor.train_models(X, y, test_size=test_size, random_state=random_state)\n",
    "    \n",
    "    if 'error' in results:\n",
    "        print(f\"Error training models: {results['error']}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Model training complete!\")\n",
    "    \n",
    "    # Prepare results for tabular display\n",
    "    model_results = []\n",
    "    for model_name, metrics in results.items():\n",
    "        accuracy = metrics.get('accuracy', 0) * 100\n",
    "        cv_mean = metrics.get('cross_val_mean', 0) * 100\n",
    "        cv_std = metrics.get('cross_val_std', 0) * 100\n",
    "        brier = metrics.get('brier_score', 0)\n",
    "        \n",
    "        model_results.append({\n",
    "            'Model': model_name.replace('_', ' ').title(),\n",
    "            'Test Accuracy': accuracy,\n",
    "            'Cross-Val Accuracy': cv_mean,\n",
    "            'Cross-Val Std': cv_std,\n",
    "            'Brier Score': brier\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame and sort by accuracy\n",
    "    results_df = pd.DataFrame(model_results).sort_values('Test Accuracy', ascending=False)\n",
    "    \n",
    "    # Generate plot of model performance\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Plot test accuracy with cross-validation range\n",
    "    ax = sns.barplot(x='Model', y='Test Accuracy', data=results_df, color='skyblue')\n",
    "    \n",
    "    # Add error bars for cross-validation\n",
    "    for i, row in results_df.iterrows():\n",
    "        ax.errorbar(i, row['Cross-Val Accuracy'], yerr=row['Cross-Val Std'], \n",
    "                   fmt='o', color='black', capsize=5)\n",
    "    \n",
    "    plt.title('Model Performance Comparison')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X, y, results, results_df\n",
    "\n",
    "# Train models if we have data\n",
    "if all_matches:\n",
    "    X, y, model_results, results_df = train_and_evaluate_models(all_matches)\n",
    "    \n",
    "    if results_df is not None:\n",
    "        display(results_df.style.format({\n",
    "            'Test Accuracy': '{:.2f}%',\n",
    "            'Cross-Val Accuracy': '{:.2f}%',\n",
    "            'Cross-Val Std': '{:.2f}%',\n",
    "            'Brier Score': '{:.4f}'\n",
    "        }))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "046b808e",
   "metadata": {},
   "source": [
    "## Feature Importance Analysis\n",
    "\n",
    "Let's examine which features are most important for predicting match outcomes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8973e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_feature_importance(model_name='ensemble', top_n=15):\n",
    "    \"\"\"Plot feature importance for a specific model\"\"\"\n",
    "    try:\n",
    "        # Get feature importance\n",
    "        ml_predictor.plot_feature_importance(model_name=model_name, top_n=top_n)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error plotting feature importance: {str(e)}\")\n",
    "        return False\n",
    "\n",
    "if 'X' in locals() and not X.empty:\n",
    "    # Plot feature importance for different models\n",
    "    models_to_check = ['xgboost', 'random_forest', 'ensemble']\n",
    "    \n",
    "    for model in models_to_check:\n",
    "        print(f\"\\nFeature importance for {model}:\")\n",
    "        plot_feature_importance(model_name=model, top_n=15)\n",
    "        \n",
    "    # Generate SHAP analysis for better interpretability\n",
    "    try:\n",
    "        print(\"\\nGenerating SHAP analysis for model interpretability...\")\n",
    "        ml_predictor.save_shap_analysis(X, model_name='xgboost')\n",
    "        print(\"SHAP analysis saved to the model directory.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not generate SHAP analysis: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb2f405",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "\n",
    "Now let's tune hyperparameters to improve model performance. This may take some time to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_hyperparameters(X, y, model_type='xgboost', n_trials=50, test_size=0.2, random_state=42):\n",
    "    \"\"\"Perform hyperparameter tuning for a specific model type\"\"\"\n",
    "    print(f\"Starting hyperparameter tuning for {model_type} model with {n_trials} trials...\")\n",
    "    \n",
    "    results = ml_predictor.hypertune_model(\n",
    "        X, y, model_type=model_type, n_trials=n_trials, \n",
    "        test_size=test_size, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    if 'error' in results:\n",
    "        print(f\"Error during hyperparameter tuning: {results['error']}\")\n",
    "        return None\n",
    "    \n",
    "    print(\"Hyperparameter tuning complete!\")\n",
    "    \n",
    "    # Print results\n",
    "    accuracy = results.get('test_accuracy', 0) * 100\n",
    "    best_cv_score = results.get('best_cv_score', 0) * 100\n",
    "    brier = results.get('brier_score', 0)\n",
    "    \n",
    "    print(f\"Best parameters: {results.get('best_params', {})}\")\n",
    "    print(f\"Test accuracy: {accuracy:.2f}%\")\n",
    "    print(f\"Best CV score: {best_cv_score:.2f}%\")\n",
    "    print(f\"Brier score: {brier:.4f}\")\n",
    "    \n",
    "    # Display classification report\n",
    "    report = results.get('classification_report', {})\n",
    "    if report:\n",
    "        report_df = pd.DataFrame(report).transpose()\n",
    "        display(report_df.style.format({\n",
    "            'precision': '{:.2f}',\n",
    "            'recall': '{:.2f}',\n",
    "            'f1-score': '{:.2f}',\n",
    "            'support': '{:.0f}'\n",
    "        }))\n",
    "    \n",
    "    # Plot optimization history\n",
    "    history = results.get('optimization_history', [])\n",
    "    if history:\n",
    "        history_df = pd.DataFrame(history)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(history_df['number'], history_df['value'], 'o-')\n",
    "        plt.title(f'{model_type.title()} Hyperparameter Optimization Progress')\n",
    "        plt.xlabel('Trial Number')\n",
    "        plt.ylabel('Accuracy')\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run hyperparameter tuning if we have data (commented by default as it's time-consuming)\n",
    "# If you want to run it, uncomment the code below\n",
    "'''\n",
    "if 'X' in locals() and not X.empty:\n",
    "    # Choose a model type to tune\n",
    "    model_type = 'xgboost'  # Options: 'xgboost', 'lightgbm', 'random_forest', 'logistic_regression'\n",
    "    n_trials = 50  # Increase for better results, but will take longer\n",
    "    \n",
    "    tuning_results = tune_hyperparameters(X, y, model_type=model_type, n_trials=n_trials)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64cca69f",
   "metadata": {},
   "source": [
    "## Backtesting System\n",
    "\n",
    "Now let's create a backtesting framework to evaluate betting strategies based on our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e935a44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backtest_betting_strategy(matches, initial_balance=1000, stake_pct=0.02, model_name='ensemble', \n",
    "                             bookmaker_weight=0.5, min_threshold=0.6, min_odds=1.5):\n",
    "    \"\"\"Simulate a betting strategy on historical data\"\"\"\n",
    "    if not matches:\n",
    "        print(\"No matches available for backtesting\")\n",
    "        return None\n",
    "    \n",
    "    print(f\"Starting backtest with {len(matches)} historical matches...\")\n",
    "    print(f\"Initial balance: ${initial_balance:.2f}, Stake: {stake_pct*100:.1f}% of balance per bet\")\n",
    "    print(f\"Model: {model_name}, Bookmaker weight: {bookmaker_weight*100:.1f}%\")\n",
    "    print(f\"Min confidence threshold: {min_threshold*100:.1f}%, Min odds: {min_odds:.2f}\")\n",
    "    \n",
    "    # Prepare for backtesting\n",
    "    results = []\n",
    "    balance = initial_balance\n",
    "    total_bets = 0\n",
    "    winning_bets = 0\n",
    "    history = [balance]\n",
    "    bet_outcomes = []\n",
    "    \n",
    "    # Sort matches by date if available\n",
    "    if all(('match_date' in match) for match in matches):\n",
    "        matches = sorted(matches, key=lambda x: x.get('match_date', ''))\n",
    "    \n",
    "    # Iterate through matches chronologically\n",
    "    for match in tqdm(matches, desc=\"Backtesting\"):\n",
    "        # Skip matches without results\n",
    "        if 'actual_result' not in match or not match['actual_result']:\n",
    "            continue\n",
    "        \n",
    "        # Get combined prediction\n",
    "        prediction = ml_predictor.combine_predictions(match, bookmaker_weight=bookmaker_weight)\n",
    "        \n",
    "        if 'error' in prediction:\n",
    "            continue\n",
    "        \n",
    "        # Determine if we should bet based on confidence\n",
    "        predicted_outcome = prediction.get('predicted_outcome')\n",
    "        confidence = prediction.get('confidence', 0)\n",
    "        actual_result = match.get('actual_result')\n",
    "        \n",
    "        # Find the odds for the predicted outcome\n",
    "        all_odds = []\n",
    "        for bookie in match.get('bookmakers', []):\n",
    "            if predicted_outcome == 'home':\n",
    "                all_odds.append(bookie.get('home_odds', 0))\n",
    "            elif predicted_outcome == 'draw':\n",
    "                all_odds.append(bookie.get('draw_odds', 0))\n",
    "            elif predicted_outcome == 'away':\n",
    "                all_odds.append(bookie.get('away_odds', 0))\n",
    "        \n",
    "        # Use the best odds available\n",
    "        best_odds = max(all_odds) if all_odds else 0\n",
    "        \n",
    "        # Check if we meet our betting criteria\n",
    "        should_bet = confidence >= min_threshold and best_odds >= min_odds\n",
    "        \n",
    "        if should_bet:\n",
    "            # Calculate stake\n",
    "            stake = balance * stake_pct\n",
    "            \n",
    "            # Determine outcome\n",
    "            won = (predicted_outcome == actual_result)\n",
    "            \n",
    "            # Update balance\n",
    "            if won:\n",
    "                profit = stake * (best_odds - 1)\n",
    "                balance += profit\n",
    "                winning_bets += 1\n",
    "            else:\n",
    "                balance -= stake\n",
    "            \n",
    "            # Record bet details\n",
    "            bet_outcomes.append({\n",
    "                'match_id': match.get('match_id', ''),\n",
    "                'home_team': match.get('home_team', ''),\n",
    "                'away_team': match.get('away_team', ''),\n",
    "                'match_date': match.get('match_date', ''),\n",
    "                'predicted': predicted_outcome,\n",
    "                'actual': actual_result,\n",
    "                'confidence': confidence,\n",
    "                'odds': best_odds,\n",
    "                'stake': stake,\n",
    "                'won': won,\n",
    "                'balance': balance\n",
    "            })\n",
    "            \n",
    "            total_bets += 1\n",
    "        \n",
    "        # Record balance history\n",
    "        history.append(balance)\n",
    "    \n",
    "    # Calculate results\n",
    "    final_balance = balance\n",
    "    roi = (final_balance - initial_balance) / initial_balance * 100\n",
    "    win_rate = winning_bets / total_bets * 100 if total_bets > 0 else 0\n",
    "    \n",
    "    print(\"\\nBacktest Results:\")\n",
    "    print(f\"Total bets: {total_bets}\")\n",
    "    print(f\"Winning bets: {winning_bets} ({win_rate:.2f}%)\")\n",
    "    print(f\"Final balance: ${final_balance:.2f}\")\n",
    "    print(f\"Profit/Loss: ${final_balance - initial_balance:.2f}\")\n",
    "    print(f\"ROI: {roi:.2f}%\")\n",
    "    \n",
    "    # Plot balance history\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(history)\n",
    "    plt.axhline(y=initial_balance, color='r', linestyle='--')\n",
    "    plt.title('Balance History')\n",
    "    plt.xlabel('Event')\n",
    "    plt.ylabel('Balance ($)')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Create DataFrame of bet results\n",
    "    if bet_outcomes:\n",
    "        bet_df = pd.DataFrame(bet_outcomes)\n",
    "        \n",
    "        # Plot bet outcomes by confidence level\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=\"won\", y=\"confidence\", data=bet_df)\n",
    "        plt.title(\"Bet Outcomes by Confidence Level\")\n",
    "        plt.xlabel(\"Bet Won\")\n",
    "        plt.ylabel(\"Confidence\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot bet outcomes by odds\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.boxplot(x=\"won\", y=\"odds\", data=bet_df)\n",
    "        plt.title(\"Bet Outcomes by Odds\")\n",
    "        plt.xlabel(\"Bet Won\")\n",
    "        plt.ylabel(\"Odds\")\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return the first few bet details\n",
    "        return bet_df\n",
    "    \n",
    "    return None\n",
    "\n",
    "# Run backtest if we have data\n",
    "if all_matches:\n",
    "    # Backtest default strategy\n",
    "    bet_results = backtest_betting_strategy(\n",
    "        all_matches, \n",
    "        initial_balance=1000, \n",
    "        stake_pct=0.02,  # 2% of balance per bet\n",
    "        model_name='ensemble',\n",
    "        bookmaker_weight=0.5,  # Equal weight to ML and bookmakers\n",
    "        min_threshold=0.6,  # Only bet when confidence is at least 60%\n",
    "        min_odds=1.5  # Only bet when odds are at least 1.5\n",
    "    )\n",
    "    \n",
    "    if bet_results is not None:\n",
    "        # Show some detailed bet information\n",
    "        display(bet_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43e5085",
   "metadata": {},
   "source": [
    "## Parameter Optimization\n",
    "\n",
    "Let's experiment with different parameters to find the optimal betting strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "782a519f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_strategy_parameters(matches, param_grid):\n",
    "    \"\"\"Run backtests with different parameter combinations to find optimal strategy\"\"\"\n",
    "    if not matches:\n",
    "        print(\"No matches available for optimization\")\n",
    "        return None\n",
    "    \n",
    "    # Prepare results tracking\n",
    "    results = []\n",
    "    \n",
    "    # Total number of combinations to test\n",
    "    n_combinations = 1\n",
    "    for param_values in param_grid.values():\n",
    "        n_combinations *= len(param_values)\n",
    "    \n",
    "    print(f\"Testing {n_combinations} parameter combinations...\")\n",
    "    \n",
    "    # Generate all parameter combinations\n",
    "    def generate_param_combinations(param_grid, current_idx=0, current_params={}):\n",
    "        param_names = list(param_grid.keys())\n",
    "        \n",
    "        if current_idx >= len(param_names):\n",
    "            return [current_params.copy()]\n",
    "        \n",
    "        current_param = param_names[current_idx]\n",
    "        combinations = []\n",
    "        \n",
    "        for value in param_grid[current_param]:\n",
    "            current_params[current_param] = value\n",
    "            combinations.extend(generate_param_combinations(param_grid, current_idx + 1, current_params))\n",
    "        \n",
    "        return combinations\n",
    "    \n",
    "    param_combinations = generate_param_combinations(param_grid)\n",
    "    \n",
    "    # Run backtest for each combination\n",
    "    for params in tqdm(param_combinations, desc=\"Optimizing\"):\n",
    "        # Fixed parameters\n",
    "        initial_balance = 1000\n",
    "        \n",
    "        # Track balance history\n",
    "        balance = initial_balance\n",
    "        total_bets = 0\n",
    "        winning_bets = 0\n",
    "        \n",
    "        # Extract parameters\n",
    "        stake_pct = params.get('stake_pct', 0.02)\n",
    "        bookmaker_weight = params.get('bookmaker_weight', 0.5)\n",
    "        min_threshold = params.get('min_threshold', 0.6)\n",
    "        min_odds = params.get('min_odds', 1.5)\n",
    "        model_name = params.get('model_name', 'ensemble')\n",
    "        \n",
    "        # Sort matches by date if available\n",
    "        if all(('match_date' in match) for match in matches):\n",
    "            sorted_matches = sorted(matches, key=lambda x: x.get('match_date', ''))\n",
    "        else:\n",
    "            sorted_matches = matches\n",
    "        \n",
    "        # Iterate through matches chronologically\n",
    "        for match in sorted_matches:\n",
    "            # Skip matches without results\n",
    "            if 'actual_result' not in match or not match['actual_result']:\n",
    "                continue\n",
    "            \n",
    "            # Get combined prediction\n",
    "            prediction = ml_predictor.combine_predictions(match, bookmaker_weight=bookmaker_weight)\n",
    "            \n",
    "            if 'error' in prediction:\n",
    "                continue\n",
    "            \n",
    "            # Determine if we should bet based on confidence\n",
    "            predicted_outcome = prediction.get('predicted_outcome')\n",
    "            confidence = prediction.get('confidence', 0)\n",
    "            actual_result = match.get('actual_result')\n",
    "            \n",
    "            # Find the odds for the predicted outcome\n",
    "            all_odds = []\n",
    "            for bookie in match.get('bookmakers', []):\n",
    "                if predicted_outcome == 'home':\n",
    "                    all_odds.append(bookie.get('home_odds', 0))\n",
    "                elif predicted_outcome == 'draw':\n",
    "                    all_odds.append(bookie.get('draw_odds', 0))\n",
    "                elif predicted_outcome == 'away':\n",
    "                    all_odds.append(bookie.get('away_odds', 0))\n",
    "            \n",
    "            # Use the best odds available\n",
    "            best_odds = max(all_odds) if all_odds else 0\n",
    "            \n",
    "            # Check if we meet our betting criteria\n",
    "            should_bet = confidence >= min_threshold and best_odds >= min_odds\n",
    "            \n",
    "            if should_bet:\n",
    "                # Calculate stake\n",
    "                stake = balance * stake_pct\n",
    "                \n",
    "                # Determine outcome\n",
    "                won = (predicted_outcome == actual_result)\n",
    "                \n",
    "                # Update balance\n",
    "                if won:\n",
    "                    profit = stake * (best_odds - 1)\n",
    "                    balance += profit\n",
    "                    winning_bets += 1\n",
    "                else:\n",
    "                    balance -= stake\n",
    "                \n",
    "                total_bets += 1\n",
    "        \n",
    "        # Calculate results\n",
    "        final_balance = balance\n",
    "        roi = (final_balance - initial_balance) / initial_balance * 100\n",
    "        win_rate = winning_bets / total_bets * 100 if total_bets > 0 else 0\n",
    "        \n",
    "        # Store the results\n",
    "        results.append({\n",
    "            **params,\n",
    "            'total_bets': total_bets,\n",
    "            'winning_bets': winning_bets,\n",
    "            'win_rate': win_rate,\n",
    "            'final_balance': final_balance,\n",
    "            'profit': final_balance - initial_balance,\n",
    "            'roi': roi\n",
    "        })\n",
    "    \n",
    "    # Convert to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Sort by ROI descending\n",
    "    results_df = results_df.sort_values('roi', ascending=False)\n",
    "    \n",
    "    print(\"\\nTop 5 Strategies:\")\n",
    "    display(results_df.head(5).style.format({\n",
    "        'stake_pct': '{:.2%}',\n",
    "        'bookmaker_weight': '{:.2f}',\n",
    "        'min_threshold': '{:.2f}',\n",
    "        'min_odds': '{:.2f}',\n",
    "        'win_rate': '{:.2f}%',\n",
    "        'final_balance': '${:.2f}',\n",
    "        'profit': '${:.2f}',\n",
    "        'roi': '{:.2f}%'\n",
    "    }))\n",
    "    \n",
    "    # Visualize parameter importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    param_names = list(param_grid.keys())\n",
    "    for i, param in enumerate(param_names):\n",
    "        plt.subplot(len(param_names), 1, i+1)\n",
    "        sns.boxplot(x=param, y='roi', data=results_df)\n",
    "        plt.title(f'ROI by {param}')\n",
    "        plt.ylabel('ROI (%)')\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Uncomment to run parameter optimization (takes significant time)\n",
    "'''\n",
    "if all_matches:\n",
    "    # Define parameter grid to search\n",
    "    param_grid = {\n",
    "        'stake_pct': [0.01, 0.02, 0.05],\n",
    "        'bookmaker_weight': [0.3, 0.5, 0.7],\n",
    "        'min_threshold': [0.55, 0.6, 0.65, 0.7],\n",
    "        'min_odds': [1.3, 1.5, 1.7, 2.0],\n",
    "        'model_name': ['ensemble', 'xgboost']\n",
    "    }\n",
    "    \n",
    "    # Run optimization\n",
    "    optimization_results = optimize_strategy_parameters(all_matches, param_grid)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bdb798f",
   "metadata": {},
   "source": [
    "## Evaluating Model Performance on Specific Match Types\n",
    "\n",
    "Let's analyze how our models perform on different types of matches (e.g., by competition, by favorite status)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd43f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_model_performance_by_category(matches, model_name='ensemble', bookmaker_weight=0.5, category='competition'):\n",
    "    \"\"\"Analyze model performance across different categories\"\"\"\n",
    "    if not matches:\n",
    "        print(\"No matches available for analysis\")\n",
    "        return None\n",
    "    \n",
    "    # Collect prediction results\n",
    "    results = []\n",
    "    \n",
    "    for match in tqdm(matches, desc=\"Analyzing matches\"):\n",
    "        # Skip matches without results\n",
    "        if 'actual_result' not in match or not match['actual_result']:\n",
    "            continue\n",
    "        \n",
    "        # Get category value\n",
    "        category_value = match.get(category, 'Unknown')\n",
    "        \n",
    "        # Get prediction\n",
    "        prediction = ml_predictor.combine_predictions(match, bookmaker_weight=bookmaker_weight)\n",
    "        \n",
    "        if 'error' in prediction:\n",
    "            continue\n",
    "        \n",
    "        # Compare prediction with actual result\n",
    "        predicted_outcome = prediction.get('predicted_outcome')\n",
    "        actual_result = match.get('actual_result')\n",
    "        correct = (predicted_outcome == actual_result)\n",
    "        \n",
    "        # Store result\n",
    "        results.append({\n",
    "            'match_id': match.get('match_id', ''),\n",
    "            'home_team': match.get('home_team', ''),\n",
    "            'away_team': match.get('away_team', ''),\n",
    "            'predicted': predicted_outcome,\n",
    "            'actual': actual_result,\n",
    "            'correct': correct,\n",
    "            'category': category_value,\n",
    "            'confidence': prediction.get('confidence', 0)\n",
    "        })\n",
    "    \n",
    "    if not results:\n",
    "        print(\"No prediction results collected\")\n",
    "        return None\n",
    "    \n",
    "    # Create DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Calculate accuracy by category\n",
    "    category_accuracy = results_df.groupby('category').agg({\n",
    "        'correct': ['mean', 'count'],\n",
    "        'confidence': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-index columns\n",
    "    category_accuracy.columns = ['accuracy', 'count', 'avg_confidence']\n",
    "    category_accuracy = category_accuracy.reset_index()\n",
    "    \n",
    "    # Sort by count descending\n",
    "    category_accuracy = category_accuracy.sort_values('count', ascending=False)\n",
    "    \n",
    "    print(f\"\\nModel performance by {category}:\")\n",
    "    display(category_accuracy.style.format({\n",
    "        'accuracy': '{:.2%}',\n",
    "        'avg_confidence': '{:.2%}'\n",
    "    }))\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Filter to categories with at least 5 matches\n",
    "    plot_data = category_accuracy[category_accuracy['count'] >= 5].copy()\n",
    "    plot_data = plot_data.sort_values('accuracy', ascending=False)\n",
    "    \n",
    "    # Create bar chart\n",
    "    bars = plt.bar(plot_data['category'], plot_data['accuracy'] * 100)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, 5, \n",
    "                f\"n={int(plot_data.iloc[i]['count'])}\", \n",
    "                ha='center', va='bottom', color='white', fontweight='bold')\n",
    "    \n",
    "    plt.title(f'Prediction Accuracy by {category.title()}')\n",
    "    plt.xlabel(category.title())\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results_df, category_accuracy\n",
    "\n",
    "# Run analysis if we have data\n",
    "if all_matches:\n",
    "    # Analyze by competition\n",
    "    competition_results, competition_accuracy = analyze_model_performance_by_category(\n",
    "        all_matches,\n",
    "        model_name='ensemble',\n",
    "        bookmaker_weight=0.5,\n",
    "        category='competition'\n",
    "    )\n",
    "    \n",
    "    # We could analyze by other categories like favorite status\n",
    "    # First, let's create a custom category for favorite status\n",
    "    for match in all_matches:\n",
    "        bookmakers = match.get('bookmakers', [])\n",
    "        if bookmakers:\n",
    "            # Calculate average probabilities\n",
    "            home_probs = [b.get('implied_home_prob', 0) for b in bookmakers]\n",
    "            draw_probs = [b.get('implied_draw_prob', 0) for b in bookmakers]\n",
    "            away_probs = [b.get('implied_away_prob', 0) for b in bookmakers]\n",
    "            \n",
    "            avg_home_prob = sum(home_probs) / len(home_probs) if home_probs else 0\n",
    "            avg_draw_prob = sum(draw_probs) / len(draw_probs) if draw_probs else 0\n",
    "            avg_away_prob = sum(away_probs) / len(away_probs) if away_probs else 0\n",
    "            \n",
    "            # Determine favorite\n",
    "            probs = [avg_home_prob, avg_draw_prob, avg_away_prob]\n",
    "            favorite_idx = probs.index(max(probs))\n",
    "            favorite_type = ['home', 'draw', 'away'][favorite_idx]\n",
    "            \n",
    "            # Add favorite category\n",
    "            match['favorite'] = favorite_type\n",
    "            \n",
    "            # Add favorite strength category\n",
    "            max_prob = max(probs)\n",
    "            if max_prob >= 0.6:\n",
    "                strength = 'strong'\n",
    "            elif max_prob >= 0.4:\n",
    "                strength = 'medium'\n",
    "            else:\n",
    "                strength = 'weak'\n",
    "            \n",
    "            match['favorite_strength'] = f\"{favorite_type}_{strength}\"\n",
    "    \n",
    "    # Analyze by favorite status\n",
    "    favorite_results, favorite_accuracy = analyze_model_performance_by_category(\n",
    "        all_matches,\n",
    "        model_name='ensemble',\n",
    "        bookmaker_weight=0.5,\n",
    "        category='favorite'\n",
    "    )\n",
    "    \n",
    "    # Analyze by favorite strength\n",
    "    favorite_strength_results, favorite_strength_accuracy = analyze_model_performance_by_category(\n",
    "        all_matches,\n",
    "        model_name='ensemble',\n",
    "        bookmaker_weight=0.5,\n",
    "        category='favorite_strength'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15cfc8c",
   "metadata": {},
   "source": [
    "## Custom Analysis Questions\n",
    "\n",
    "Let's answer some specific analysis questions to gain deeper insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "385740c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's look at how accuracy changes with different probability thresholds\n",
    "if 'competition_results' in locals() and not competition_results.empty:\n",
    "    # Create bins of confidence levels\n",
    "    bins = [0, 0.55, 0.6, 0.65, 0.7, 0.75, 0.8, 0.85, 0.9, 1.0]\n",
    "    labels = ['<55%', '55-60%', '60-65%', '65-70%', '70-75%', '75-80%', '80-85%', '85-90%', '90-100%']\n",
    "    \n",
    "    competition_results['confidence_bin'] = pd.cut(competition_results['confidence'], bins=bins, labels=labels)\n",
    "    \n",
    "    # Calculate accuracy by confidence bin\n",
    "    confidence_accuracy = competition_results.groupby('confidence_bin').agg({\n",
    "        'correct': ['mean', 'count'],\n",
    "        'confidence': 'mean'\n",
    "    })\n",
    "    \n",
    "    # Flatten multi-index columns\n",
    "    confidence_accuracy.columns = ['accuracy', 'count', 'avg_confidence']\n",
    "    confidence_accuracy = confidence_accuracy.reset_index()\n",
    "    \n",
    "    print(\"\\nAccuracy by confidence threshold:\")\n",
    "    display(confidence_accuracy.style.format({\n",
    "        'accuracy': '{:.2%}',\n",
    "        'avg_confidence': '{:.2%}'\n",
    "    }))\n",
    "    \n",
    "    # Plot results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    bars = plt.bar(confidence_accuracy['confidence_bin'], confidence_accuracy['accuracy'] * 100)\n",
    "    \n",
    "    # Add count labels\n",
    "    for i, bar in enumerate(bars):\n",
    "        plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "                f\"n={int(confidence_accuracy.iloc[i]['count'])}\", \n",
    "                ha='center', va='bottom')\n",
    "    \n",
    "    plt.title('Prediction Accuracy by Confidence Level')\n",
    "    plt.xlabel('Confidence Range')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.ylim(0, 100)\n",
    "    plt.grid(axis='y')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31dac85a",
   "metadata": {},
   "source": [
    "## Export a Trained Model for Production Use\n",
    "\n",
    "Finally, let's export our best trained model for production use in the main application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0844d608",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model_info(model_name='ensemble', bookmaker_weight=0.5):\n",
    "    \"\"\"Export model information and recommended parameters for production use\"\"\"\n",
    "    export_info = {\n",
    "        'model_name': model_name,\n",
    "        'bookmaker_weight': bookmaker_weight,\n",
    "        'description': f\"ML model trained on {datetime.now().strftime('%Y-%m-%d')}\",\n",
    "        'recommended_threshold': 0.65,  # This can be adjusted based on your analysis\n",
    "        'recommended_min_odds': 1.5,    # This can be adjusted based on your analysis\n",
    "        'notes': \"Created using backtesting notebook\"\n",
    "    }\n",
    "    \n",
    "    # Save to a file\n",
    "    os.makedirs(os.path.join(data_dir, 'ml_exports'), exist_ok=True)\n",
    "    export_path = os.path.join(data_dir, 'ml_exports', 'model_info.json')\n",
    "    \n",
    "    with open(export_path, 'w') as f:\n",
    "        json.dump(export_info, f, indent=2)\n",
    "    \n",
    "    print(f\"Model information exported to {export_path}\")\n",
    "    return export_info\n",
    "\n",
    "# Export model info if we've successfully trained models\n",
    "if 'X' in locals() and not X.empty:\n",
    "    # Use parameters from your analysis\n",
    "    export_info = export_model_info(model_name='ensemble', bookmaker_weight=0.5)\n",
    "    export_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02f72829",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've:\n",
    "\n",
    "1. Loaded and analyzed historical match data\n",
    "2. Trained multiple machine learning models to predict match outcomes\n",
    "3. Performed hyperparameter tuning to optimize model performance\n",
    "4. Backtested various betting strategies on historical data\n",
    "5. Analyzed model performance across different categories\n",
    "6. Exported trained models for production use\n",
    "\n",
    "You can use this notebook as a starting point for your own analysis and model development. As you collect more match data, you can rerun this notebook to refine your models and strategies."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
